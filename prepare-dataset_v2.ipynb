{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2074, which is longer than the specified 2000\n",
      "Created a chunk of size 2383, which is longer than the specified 2000\n",
      "Created a chunk of size 3902, which is longer than the specified 2000\n",
      "Created a chunk of size 2486, which is longer than the specified 2000\n",
      "Created a chunk of size 3295, which is longer than the specified 2000\n",
      "Created a chunk of size 2735, which is longer than the specified 2000\n",
      "Created a chunk of size 2203, which is longer than the specified 2000\n",
      "Created a chunk of size 2139, which is longer than the specified 2000\n",
      "Created a chunk of size 2973, which is longer than the specified 2000\n",
      "Created a chunk of size 2926, which is longer than the specified 2000\n",
      "Created a chunk of size 2135, which is longer than the specified 2000\n",
      "Created a chunk of size 2225, which is longer than the specified 2000\n",
      "Created a chunk of size 2345, which is longer than the specified 2000\n",
      "Created a chunk of size 2082, which is longer than the specified 2000\n",
      "Created a chunk of size 3782, which is longer than the specified 2000\n",
      "Created a chunk of size 4434, which is longer than the specified 2000\n",
      "Created a chunk of size 3317, which is longer than the specified 2000\n",
      "Created a chunk of size 2581, which is longer than the specified 2000\n",
      "Created a chunk of size 3224, which is longer than the specified 2000\n",
      "Created a chunk of size 2906, which is longer than the specified 2000\n",
      "Created a chunk of size 2420, which is longer than the specified 2000\n",
      "Created a chunk of size 3093, which is longer than the specified 2000\n",
      "Created a chunk of size 2729, which is longer than the specified 2000\n",
      "Created a chunk of size 6961, which is longer than the specified 2000\n",
      "Created a chunk of size 2028, which is longer than the specified 2000\n",
      "Created a chunk of size 2314, which is longer than the specified 2000\n",
      "Created a chunk of size 2044, which is longer than the specified 2000\n",
      "Created a chunk of size 2070, which is longer than the specified 2000\n",
      "Created a chunk of size 2668, which is longer than the specified 2000\n",
      "Created a chunk of size 2276, which is longer than the specified 2000\n",
      "Created a chunk of size 2251, which is longer than the specified 2000\n",
      "Created a chunk of size 2502, which is longer than the specified 2000\n",
      "Created a chunk of size 2343, which is longer than the specified 2000\n",
      "Created a chunk of size 2834, which is longer than the specified 2000\n",
      "Created a chunk of size 2245, which is longer than the specified 2000\n",
      "Created a chunk of size 2152, which is longer than the specified 2000\n",
      "Created a chunk of size 2080, which is longer than the specified 2000\n",
      "Created a chunk of size 2034, which is longer than the specified 2000\n",
      "Created a chunk of size 2166, which is longer than the specified 2000\n",
      "Created a chunk of size 2012, which is longer than the specified 2000\n",
      "Created a chunk of size 2450, which is longer than the specified 2000\n",
      "Created a chunk of size 2441, which is longer than the specified 2000\n",
      "Created a chunk of size 2064, which is longer than the specified 2000\n",
      "Processing chunks: 100%|██████████| 1379/1379 [1:09:20<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Xử lý xong:\n",
      "- Tổng số mẫu đã xử lý: 4138\n",
      "- Tổng số token đã sử dụng: 812352\n",
      "- Tổng số input tokens: 670627\n",
      "- Tổng số output tokens: 141725\n",
      "- Chi phí cho input tokens: $0.101\n",
      "- Chi phí cho output tokens: $0.085\n",
      "- Tổng chi phí: $0.186\n",
      "- Thời gian thực hiện: 4160.31 giây\n",
      "- Dataset được lưu tại: output_samples.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "from underthesea import sent_tokenize\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Thiết lập API key cho OpenAI từ biến môi trường\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Kiểm tra xem API key có được thiết lập hay không\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"API key chưa được thiết lập. Vui lòng thiết lập biến môi trường 'OPENAI_API_KEY'.\")\n",
    "\n",
    "class VietnameseTextProcessor:\n",
    "    def __init__(self, max_length: int = 2000):\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def read_text_files(self, input_folder: str) -> str:\n",
    "        \"\"\"Đọc tất cả các file văn bản trong thư mục.\"\"\"\n",
    "        all_texts = []\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                    all_texts.append(f.read())\n",
    "        return \"\\n\".join(all_texts)\n",
    "\n",
    "    def create_chunks(self, text: str) -> list:\n",
    "        \"\"\"Tạo chunk từ văn bản.\"\"\"\n",
    "        splitter = CharacterTextSplitter(\n",
    "            chunk_size=self.max_length,\n",
    "            chunk_overlap=200,\n",
    "            separator=\"\\n\",\n",
    "            length_function=len\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def generate_questions(self, chunk: str) -> list:\n",
    "        \"\"\"Gửi chunk tới GPT-4o-mini để tạo câu hỏi.\"\"\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Bạn là một trợ lý hữu ích, am hiểu về Phật Pháp.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Hãy tạo ra 3 câu hỏi sâu sắc và khuyến khích suy nghĩ từ đoạn văn sau: {chunk}. \n",
    "                    Đoạn văn đó chính là câu trả lời cho các câu hỏi.\n",
    "                    Đảm bảo rằng các câu hỏi không chỉ lặp lại thông tin mà còn khám phá các khía cạnh khác nhau của nội dung. \n",
    "                    Vui lòng trả về kết quả dưới dạng JSON như sau:\n",
    "                    {{\n",
    "                        \"questions\": [\n",
    "                            \"Câu hỏi 1\",\n",
    "                            \"Câu hỏi 2\",\n",
    "                            \"Câu hỏi 3\"\n",
    "                        ]\n",
    "                    }}\n",
    "                \"\"\"}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"} \n",
    "        )\n",
    "\n",
    "        questions_json = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # print(\"Nội dung phản hồi từ API:\", questions_json)  # Thêm dòng này để kiểm tra\n",
    "\n",
    "        # Kiểm tra xem nội dung có hợp lệ không\n",
    "        if not questions_json:\n",
    "            raise ValueError(\"Nội dung phản hồi trống.\")\n",
    "\n",
    "        # Chuyển đổi chuỗi JSON thành danh sách\n",
    "        try:\n",
    "            questions = json.loads(questions_json)[\"questions\"]\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Lỗi phân tích cú pháp JSON:\", e)\n",
    "            raise ValueError(\"Nội dung phản hồi không phải là JSON hợp lệ.\")\n",
    "\n",
    "        return questions\n",
    "\n",
    "    def create_samples(self, questions: list, chunk: str) -> list:\n",
    "        \"\"\"Tạo sample theo định dạng Alpaca.\"\"\"\n",
    "        samples = []\n",
    "        for question in questions:\n",
    "            sample = {\n",
    "                \"instruction\": \"Hãy trả lời theo phong cách của thiền sư Thích Nhất Hạnh.\",\n",
    "                \"input\": question,\n",
    "                \"output\": chunk\n",
    "            }\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "def main():\n",
    "    processor = VietnameseTextProcessor(max_length=2000)\n",
    "\n",
    "    input_folder = \"data-raw/Thich Nhat Hanh\"\n",
    "    output_file = \"output_samples.json\"\n",
    "\n",
    "    all_samples = []\n",
    "    total_tokens = 0\n",
    "    total_samples_processed = 0\n",
    "\n",
    "    # Đọc tất cả văn bản từ các file trong thư mục\n",
    "    full_text = processor.read_text_files(input_folder)\n",
    "\n",
    "    # Tạo chunks từ văn bản\n",
    "    chunks = processor.create_chunks(full_text)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Đơn giá cho input và output tokens\n",
    "    input_price_per_million = 0.150 \n",
    "    output_price_per_million = 0.600 \n",
    "\n",
    "    # Khởi tạo tổng số token đầu vào và đầu ra\n",
    "    total_request_tokens = 0\n",
    "    total_response_tokens = 0\n",
    "\n",
    "    # Xử lý từng chunk và tạo câu hỏi\n",
    "    for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "        questions = processor.generate_questions(chunk)\n",
    "\n",
    "        # Tính toán số token cho request và response\n",
    "        request_tokens = len(chunk.split()) + sum(len(q.split()) for q in questions)\n",
    "        response_tokens = sum(len(q.split()) for q in questions)\n",
    "\n",
    "        total_request_tokens += request_tokens\n",
    "        total_response_tokens += response_tokens\n",
    "\n",
    "        total_tokens += (request_tokens + response_tokens)\n",
    "\n",
    "        # Tạo sample cho từng câu hỏi\n",
    "        samples = processor.create_samples(questions, chunk)\n",
    "        all_samples.extend(samples)\n",
    "\n",
    "        total_samples_processed += len(samples)\n",
    "\n",
    "    # Tính toán chi phí dựa trên số token đã sử dụng\n",
    "    input_cost = (total_request_tokens / 1000000) * input_price_per_million\n",
    "    output_cost = (total_response_tokens / 1000000) * output_price_per_million\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    # Lưu tất cả sample vào file JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_samples, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"\\nXử lý xong:\")\n",
    "    print(f\"- Tổng số mẫu đã xử lý: {total_samples_processed}\")\n",
    "    print(f\"- Tổng số token đã sử dụng: {total_tokens}\")\n",
    "    print(f\"- Tổng số input tokens: {total_request_tokens}\")\n",
    "    print(f\"- Tổng số output tokens: {total_response_tokens}\")\n",
    "    print(f\"- Chi phí cho input tokens: ${input_cost:.3f}\")\n",
    "    print(f\"- Chi phí cho output tokens: ${output_cost:.3f}\")\n",
    "    print(f\"- Tổng chi phí: ${total_cost:.3f}\")\n",
    "    print(f\"- Thời gian thực hiện: {end_time - start_time:.2f} giây\")\n",
    "    print(f\"- Dataset được lưu tại: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
