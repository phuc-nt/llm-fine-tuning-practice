{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "from underthesea import sent_tokenize\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Thiết lập API key cho OpenAI từ biến môi trường\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Kiểm tra xem API key có được thiết lập hay không\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"API key chưa được thiết lập. Vui lòng thiết lập biến môi trường 'OPENAI_API_KEY'.\")\n",
    "\n",
    "class VietnameseTextProcessor:\n",
    "    def __init__(self, max_length: int = 5000, overlap: int = 500):\n",
    "        self.max_length = max_length\n",
    "        self.overlap = overlap\n",
    "        \n",
    "    def read_text_files(self, input_folder: str) -> list:\n",
    "        \"\"\"Đọc tất cả các file văn bản trong thư mục.\"\"\"\n",
    "        all_texts = []\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                    all_texts.append(f.read())\n",
    "        return all_texts  # Trả về danh sách các văn bản\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Tiền xử lý văn bản: giữ nguyên định dạng và dấu câu.\"\"\"\n",
    "        text = text.replace('\\r\\n', '\\n')\n",
    "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "        return text\n",
    "    \n",
    "    def split_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"Tách văn bản thành các câu sử dụng Underthesea.\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        valid_sentences = [s.strip() for s in sentences if len(s.split()) > 5]\n",
    "        return valid_sentences\n",
    "    \n",
    "    def create_chunks(self, sentences: list) -> list:\n",
    "        \"\"\"Ghép các câu thành chunk sử dụng LangChain.\"\"\"\n",
    "        splitter = CharacterTextSplitter(\n",
    "            chunk_size=self.max_length,\n",
    "            chunk_overlap=self.overlap,\n",
    "            separator=\"\\n\",\n",
    "            length_function=len\n",
    "        )\n",
    "        text = '\\n'.join(sentences)\n",
    "        chunks = splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    def generate_questions_and_answers(self, chunk: str) -> dict:\n",
    "        \"\"\"Gửi chunk tới GPT-4o-mini để tạo câu hỏi và câu trả lời.\"\"\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Bạn am hiểu về Phật Pháp, Thiền Học của Thiền Sư Thích Nhất Hạnh.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Từ \"nội dung sách\", hãy đặt 5 câu hỏi.\n",
    "                    Đóng vai trò là Phật Tử tu học, muốn được Thiền Sư Thích Nhất Hạnh giảng dạy để đặt câu hỏi.\n",
    "                    Các câu hỏi đảm bảo rõ ràng, cụ thể, phản ánh và khai thác đầy đủ nội dung sách.\n",
    "                    Các câu trả lời, hãy đóng vai Thiền Sư Thích Nhất Hạnh để trả lời dựa trên nội dung sách.\n",
    "                    Đảm bảo câu trả lời mô phỏng được văn phong của nội dung sách, sử dụng nhiều thông tin nhất có thể, lặp lại từ các câu trả lời khác cũng được.\n",
    "                    Hãy trả về kết quả dưới dạng JSON như sau:\n",
    "                    {{\n",
    "                        \"questions\": [\n",
    "                            \"nội dung câu hỏi\"\n",
    "                        ],\n",
    "                        ,\n",
    "                        \"answers\": [\n",
    "                            \"nội dung câu trả lời\"\n",
    "                        ]\n",
    "                    }}\n",
    "                    Nội dung sách: {chunk}\n",
    "                \"\"\"},\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        questions_answers_json = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Kiểm tra xem nội dung có hợp lệ không\n",
    "        if not questions_answers_json:\n",
    "            raise ValueError(\"Nội dung phản hồi trống.\")\n",
    "\n",
    "        # Chuyển đổi chuỗi JSON thành danh sách\n",
    "        try:\n",
    "            data = json.loads(questions_answers_json)\n",
    "            questions = data[\"questions\"]\n",
    "            answers = data[\"answers\"]\n",
    "            return {\"questions\": questions, \"answers\": answers}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Lỗi phân tích cú pháp JSON:\", e)\n",
    "            raise ValueError(\"Nội dung phản hồi không phải là JSON hợp lệ.\")\n",
    "\n",
    "    def create_samples(self, questions: list, answers: list) -> list:\n",
    "        \"\"\"Tạo sample theo định dạng Alpaca.\"\"\"\n",
    "        samples = []\n",
    "        for question, answer in zip(questions, answers):\n",
    "            sample = {\n",
    "                \"instruction\": \"Hãy trả lời theo văn phong của Thiền Sư Thích Nhất Hạnh.\",\n",
    "                \"input\": question,\n",
    "                \"output\": answer\n",
    "            }\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "def main():\n",
    "    processor = VietnameseTextProcessor(max_length=5000)\n",
    "\n",
    "    input_folder = \"data-raw/Thich Nhat Hanh 2\"\n",
    "    output_file = \"output_samples_2.json\"\n",
    "    log_file = \"error_log.log\"  # Tên file log để ghi lại lỗi\n",
    "\n",
    "    all_samples = []\n",
    "    total_tokens = 0\n",
    "    total_samples_processed = 0\n",
    "\n",
    "    # Đọc tất cả văn bản từ các file trong thư mục\n",
    "    all_texts = processor.read_text_files(input_folder)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Đơn giá cho input và output tokens\n",
    "    input_price_per_million = 0.150 \n",
    "    output_price_per_million = 0.600 \n",
    "\n",
    "    # Khởi tạo tổng số token đầu vào và đầu ra\n",
    "    total_request_tokens = 0\n",
    "    total_response_tokens = 0\n",
    "\n",
    "    # Xử lý từng file sách\n",
    "    for idx, full_text in enumerate(all_texts):\n",
    "        # Tiền xử lý văn bản\n",
    "        processed_text = processor.preprocess_text(full_text)\n",
    "\n",
    "        # Tách văn bản thành các câu\n",
    "        sentences = processor.split_into_sentences(processed_text)\n",
    "\n",
    "        # Tạo chunks từ các câu đã tách\n",
    "        chunks = processor.create_chunks(sentences)\n",
    "\n",
    "        # Xử lý từng chunk và tạo câu hỏi cùng câu trả lời\n",
    "        for i, chunk in tqdm(enumerate(chunks), desc=f\"Đang xử lý sách {idx + 1}\", total=len(chunks)):\n",
    "            try:\n",
    "                qa_data = processor.generate_questions_and_answers(chunk)\n",
    "\n",
    "                # Tính toán số token cho request và response\n",
    "                request_tokens = len(chunk.split()) + sum(len(q.split()) for q in qa_data[\"questions\"])\n",
    "                response_tokens = sum(len(a.split()) for a in qa_data[\"answers\"])\n",
    "\n",
    "                total_request_tokens += request_tokens\n",
    "                total_response_tokens += response_tokens\n",
    "\n",
    "                total_tokens += (request_tokens + response_tokens)\n",
    "\n",
    "                # Tạo sample cho từng câu hỏi và câu trả lời\n",
    "                samples = processor.create_samples(qa_data[\"questions\"], qa_data[\"answers\"])\n",
    "                \n",
    "                # Lưu từng sample vào file JSON ngay sau khi tạo\n",
    "                for sample in samples:\n",
    "                    all_samples.append(sample)\n",
    "                    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                        json.dump(sample, f, ensure_ascii=False)\n",
    "                        f.write('\\n')  # Ghi thêm dòng mới sau mỗi sample\n",
    "\n",
    "                total_samples_processed += len(samples)\n",
    "\n",
    "            except Exception as e:\n",
    "                with open(log_file, 'a', encoding='utf-8') as log_f:\n",
    "                    log_f.write(f\"Error processing chunk {i} of book {idx + 1}: {str(e)}\\n\")\n",
    "                print(f\"Đã xảy ra lỗi với chunk {i + 1} của sách {idx + 1}: {str(e)}\")\n",
    "\n",
    "    # Tính toán chi phí dựa trên số token đã sử dụng\n",
    "    input_cost = (total_request_tokens / 1000000) * input_price_per_million\n",
    "    output_cost = (total_response_tokens / 1000000) * output_price_per_million\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"\\nXử lý xong:\")\n",
    "    print(f\"- Tổng số mẫu đã xử lý: {total_samples_processed}\")\n",
    "    print(f\"- Tổng số token đã sử dụng: {total_tokens}\")\n",
    "    print(f\"- Tổng số input tokens: {total_request_tokens}\")\n",
    "    print(f\"- Tổng số output tokens: {total_response_tokens}\")\n",
    "    print(f\"- Chi phí cho input tokens: ${input_cost:.3f}\")\n",
    "    print(f\"- Chi phí cho output tokens: ${output_cost:.3f}\")\n",
    "    print(f\"- Tổng chi phí: ${total_cost:.3f}\")\n",
    "    print(f\"- Thời gian thực hiện: {end_time - start_time:.2f} giây\")\n",
    "    print(f\"- Dataset được lưu tại: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'output_samples.json' đã được sửa thành công.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def fix_json_file(file_path):\n",
    "    try:\n",
    "        # Đọc nội dung từ file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Xóa các dòng trống và tạo danh sách các đối tượng JSON\n",
    "        json_objects = [json.loads(line) for line in lines if line.strip()]\n",
    "\n",
    "        # Ghi lại vào file dưới dạng mảng JSON\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_objects, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"File '{file_path}' đã được sửa thành công.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi khi sửa file: {e}\")\n",
    "\n",
    "# Gọi hàm với đường dẫn tới file cần sửa\n",
    "fix_json_file('output_samples.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
