{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Đang xử lý sách 1: 100%|██████████| 114/114 [11:49<00:00,  6.23s/it]\n",
      "Đang xử lý sách 2: 100%|██████████| 106/106 [10:36<00:00,  6.01s/it]\n",
      "Đang xử lý sách 3: 100%|██████████| 66/66 [06:49<00:00,  6.21s/it]\n",
      "Đang xử lý sách 4: 100%|██████████| 227/227 [22:11<00:00,  5.86s/it]\n",
      "Đang xử lý sách 5: 100%|██████████| 313/313 [30:26<00:00,  5.83s/it]\n",
      "Đang xử lý sách 6: 100%|██████████| 541/541 [47:29<00:00,  5.27s/it] \n",
      "Đang xử lý sách 7: 100%|██████████| 182/182 [16:04<00:00,  5.30s/it]\n",
      "Đang xử lý sách 8: 100%|██████████| 34/34 [03:09<00:00,  5.58s/it]\n",
      "Đang xử lý sách 9: 100%|██████████| 85/85 [07:43<00:00,  5.45s/it]\n",
      "Đang xử lý sách 10: 100%|██████████| 98/98 [10:23<00:00,  6.37s/it]\n",
      "Đang xử lý sách 11: 100%|██████████| 29/29 [02:44<00:00,  5.67s/it]\n",
      "Đang xử lý sách 12: 100%|██████████| 407/407 [40:53<00:00,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Xử lý xong:\n",
      "- Tổng số mẫu đã xử lý: 6632\n",
      "- Tổng số token đã sử dụng: 1520771\n",
      "- Tổng số input tokens: 1068841\n",
      "- Tổng số output tokens: 451930\n",
      "- Chi phí cho input tokens: $0.160\n",
      "- Chi phí cho output tokens: $0.271\n",
      "- Tổng chi phí: $0.431\n",
      "- Thời gian thực hiện: 12623.06 giây\n",
      "- Dataset được lưu tại: output_samples.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "from underthesea import sent_tokenize\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Thiết lập API key cho OpenAI từ biến môi trường\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Kiểm tra xem API key có được thiết lập hay không\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"API key chưa được thiết lập. Vui lòng thiết lập biến môi trường 'OPENAI_API_KEY'.\")\n",
    "\n",
    "class VietnameseTextProcessor:\n",
    "    def __init__(self, max_length: int = 2000, overlap: int = 200):\n",
    "        self.max_length = max_length\n",
    "        self.overlap = overlap\n",
    "        \n",
    "    def read_text_files(self, input_folder: str) -> list:\n",
    "        \"\"\"Đọc tất cả các file văn bản trong thư mục.\"\"\"\n",
    "        all_texts = []\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                    all_texts.append(f.read())\n",
    "        return all_texts  # Trả về danh sách các văn bản\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Tiền xử lý văn bản: giữ nguyên định dạng và dấu câu.\"\"\"\n",
    "        text = text.replace('\\r\\n', '\\n')\n",
    "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "        return text\n",
    "    \n",
    "    def split_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"Tách văn bản thành các câu sử dụng Underthesea.\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        valid_sentences = [s.strip() for s in sentences if len(s.split()) > 5]\n",
    "        return valid_sentences\n",
    "    \n",
    "    def create_chunks(self, sentences: list) -> list:\n",
    "        \"\"\"Ghép các câu thành chunk sử dụng LangChain.\"\"\"\n",
    "        splitter = CharacterTextSplitter(\n",
    "            chunk_size=self.max_length,\n",
    "            chunk_overlap=self.overlap,\n",
    "            separator=\"\\n\",\n",
    "            length_function=len\n",
    "        )\n",
    "        text = '\\n'.join(sentences)\n",
    "        chunks = splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    def generate_questions_and_answers(self, chunk: str) -> dict:\n",
    "        \"\"\"Gửi chunk tới GPT-4o-mini để tạo câu hỏi và câu trả lời.\"\"\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Bạn là một trợ lý hữu ích, am hiểu về Lịch sử Việt Nam giai đoạn 1975.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Đây là đoạn trích từ sách về Lịch sử Việt Nam giai đoạn 1975: {chunk} \n",
    "                    Từ đoạn trích, đóng vai người hỏi, hãy tạo 5 câu hỏi cho nội dung đoạn trích.\n",
    "                    Đảm bảo rằng các câu hỏi có thể khám phá các khía cạnh khác nhau và đầy đủ nhất của nội dung. \n",
    "                    Từ các câu hỏi, đóng vai người trả lời, hãy tạo các câu trả tương ứng từ đoạn trích.\n",
    "                    Đảm bảo chính xác thông tin của đoạn văn gốc nhất có thể.\n",
    "                    Câu trả lời dài và đầy đủ nhất có thể, nhưng không dài hơn đoạn văn gốc.\n",
    "                    Hãy trả về kết quả dưới dạng JSON như sau:\n",
    "                    {{\n",
    "                        \"questions\": [\n",
    "                            \"Câu hỏi 1\",\n",
    "                            \"Câu hỏi 2\"\n",
    "                        ],\n",
    "                        \"answers\": [\n",
    "                            \"Câu trả lời 1\",\n",
    "                            \"Câu trả lời 2\"\n",
    "                        ]\n",
    "                    }}\n",
    "                \"\"\"},\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        questions_answers_json = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Kiểm tra xem nội dung có hợp lệ không\n",
    "        if not questions_answers_json:\n",
    "            raise ValueError(\"Nội dung phản hồi trống.\")\n",
    "\n",
    "        # Chuyển đổi chuỗi JSON thành danh sách\n",
    "        try:\n",
    "            data = json.loads(questions_answers_json)\n",
    "            questions = data[\"questions\"]\n",
    "            answers = data[\"answers\"]\n",
    "            return {\"questions\": questions, \"answers\": answers}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Lỗi phân tích cú pháp JSON:\", e)\n",
    "            raise ValueError(\"Nội dung phản hồi không phải là JSON hợp lệ.\")\n",
    "\n",
    "    def create_samples(self, questions: list, answers: list) -> list:\n",
    "        \"\"\"Tạo sample theo định dạng Alpaca.\"\"\"\n",
    "        samples = []\n",
    "        for question, answer in zip(questions, answers):\n",
    "            sample = {\n",
    "                \"instruction\": \"Hãy trả lời theo phong cách của thiền sư Thích Nhất Hạnh.\",\n",
    "                \"input\": question,\n",
    "                \"output\": answer\n",
    "            }\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "def main():\n",
    "    processor = VietnameseTextProcessor(max_length=2000)\n",
    "\n",
    "    input_folder = \"data-raw/Huy Duc\"\n",
    "    output_file = \"output_samples.json\"\n",
    "    log_file = \"error_log.log\"  # Tên file log để ghi lại lỗi\n",
    "\n",
    "    all_samples = []\n",
    "    total_tokens = 0\n",
    "    total_samples_processed = 0\n",
    "\n",
    "    # Đọc tất cả văn bản từ các file trong thư mục\n",
    "    all_texts = processor.read_text_files(input_folder)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Đơn giá cho input và output tokens\n",
    "    input_price_per_million = 0.150 \n",
    "    output_price_per_million = 0.600 \n",
    "\n",
    "    # Khởi tạo tổng số token đầu vào và đầu ra\n",
    "    total_request_tokens = 0\n",
    "    total_response_tokens = 0\n",
    "\n",
    "    # Xử lý từng file sách\n",
    "    for idx, full_text in enumerate(all_texts):\n",
    "        # Tiền xử lý văn bản\n",
    "        processed_text = processor.preprocess_text(full_text)\n",
    "\n",
    "        # Tách văn bản thành các câu\n",
    "        sentences = processor.split_into_sentences(processed_text)\n",
    "\n",
    "        # Tạo chunks từ các câu đã tách\n",
    "        chunks = processor.create_chunks(sentences)\n",
    "\n",
    "        # Xử lý từng chunk và tạo câu hỏi cùng câu trả lời\n",
    "        for i, chunk in tqdm(enumerate(chunks), desc=f\"Đang xử lý sách {idx + 1}\", total=len(chunks)):\n",
    "            try:\n",
    "                qa_data = processor.generate_questions_and_answers(chunk)\n",
    "\n",
    "                # Tính toán số token cho request và response\n",
    "                request_tokens = len(chunk.split()) + sum(len(q.split()) for q in qa_data[\"questions\"])\n",
    "                response_tokens = sum(len(a.split()) for a in qa_data[\"answers\"])\n",
    "\n",
    "                total_request_tokens += request_tokens\n",
    "                total_response_tokens += response_tokens\n",
    "\n",
    "                total_tokens += (request_tokens + response_tokens)\n",
    "\n",
    "                # Tạo sample cho từng câu hỏi và câu trả lời\n",
    "                samples = processor.create_samples(qa_data[\"questions\"], qa_data[\"answers\"])\n",
    "                \n",
    "                # Lưu từng sample vào file JSON ngay sau khi tạo\n",
    "                for sample in samples:\n",
    "                    all_samples.append(sample)\n",
    "                    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                        json.dump(sample, f, ensure_ascii=False)\n",
    "                        f.write('\\n')  # Ghi thêm dòng mới sau mỗi sample\n",
    "\n",
    "                total_samples_processed += len(samples)\n",
    "\n",
    "            except Exception as e:\n",
    "                with open(log_file, 'a', encoding='utf-8') as log_f:\n",
    "                    log_f.write(f\"Error processing chunk {i} of book {idx + 1}: {str(e)}\\n\")\n",
    "                print(f\"Đã xảy ra lỗi với chunk {i + 1} của sách {idx + 1}: {str(e)}\")\n",
    "\n",
    "    # Tính toán chi phí dựa trên số token đã sử dụng\n",
    "    input_cost = (total_request_tokens / 1000000) * input_price_per_million\n",
    "    output_cost = (total_response_tokens / 1000000) * output_price_per_million\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"\\nXử lý xong:\")\n",
    "    print(f\"- Tổng số mẫu đã xử lý: {total_samples_processed}\")\n",
    "    print(f\"- Tổng số token đã sử dụng: {total_tokens}\")\n",
    "    print(f\"- Tổng số input tokens: {total_request_tokens}\")\n",
    "    print(f\"- Tổng số output tokens: {total_response_tokens}\")\n",
    "    print(f\"- Chi phí cho input tokens: ${input_cost:.3f}\")\n",
    "    print(f\"- Chi phí cho output tokens: ${output_cost:.3f}\")\n",
    "    print(f\"- Tổng chi phí: ${total_cost:.3f}\")\n",
    "    print(f\"- Thời gian thực hiện: {end_time - start_time:.2f} giây\")\n",
    "    print(f\"- Dataset được lưu tại: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "3    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'output_samples.json' đã được sửa thành công.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def fix_json_file(file_path):\n",
    "    try:\n",
    "        # Đọc nội dung từ file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Xóa các dòng trống và tạo danh sách các đối tượng JSON\n",
    "        json_objects = [json.loads(line) for line in lines if line.strip()]\n",
    "\n",
    "        # Ghi lại vào file dưới dạng mảng JSON\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_objects, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"File '{file_path}' đã được sửa thành công.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi khi sửa file: {e}\")\n",
    "\n",
    "# Gọi hàm với đường dẫn tới file cần sửa\n",
    "fix_json_file('output_samples.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
